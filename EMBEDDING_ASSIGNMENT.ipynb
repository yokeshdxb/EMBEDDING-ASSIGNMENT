{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install required packages\n",
        "!pip install --quiet \"numpy==1.26.4\" \"gensim==4.3.3\" transformers datasets nltk\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_Z5pzV-gHXD",
        "outputId": "101c77e1-7446-4270-c60b-7b8c2ced8a26"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Imports\n",
        "from datasets import load_dataset\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import numpy as np\n",
        "import gensim.downloader as api"
      ],
      "metadata": {
        "id": "tJUhJ9QEgPg5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Load Dataset and Preprocess\n",
        "dataset = load_dataset('imdb', split='train').select(range(1000))\n",
        "texts = dataset['text']\n",
        "\n",
        "def preprocess(text):\n",
        "    return \" \".join(word_tokenize(text.lower()))\n",
        "\n",
        "clean_texts = [preprocess(t) for t in texts]\n",
        "tokenized_texts = [t.split() for t in clean_texts]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rz7cRWp-gUXx",
        "outputId": "c402fbf3-44ec-44db-c1a6-d83370a45c34"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Bag of Words & TF-IDF\n",
        "bow = CountVectorizer(max_features=5000)\n",
        "bow_matrix = bow.fit_transform(clean_texts)\n",
        "print(\"BoW shape:\", bow_matrix.shape)\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "tfidf_matrix = tfidf.fit_transform(clean_texts)\n",
        "print(\"TF-IDF shape:\", tfidf_matrix.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bob5o1VWgb5p",
        "outputId": "eb3a80a1-3a4f-4fd6-f2aa-f45c36e3dfd9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BoW shape: (1000, 5000)\n",
            "TF-IDF shape: (1000, 5000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: BERT Embeddings\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def get_embedding(text):\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "\n",
        "bert_embeddings = np.vstack([get_embedding(t) for t in clean_texts[:5]])\n",
        "print(\"BERT Embeddings shape:\", bert_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUcnBiahgg6Z",
        "outputId": "3604b533-b364-4f20-f390-64a1f36ef3b0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT Embeddings shape: (5, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Pretrained Word2Vec (Google News)\n",
        "w2v_model = api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "def avg_word2vec(tokens):\n",
        "    vecs = [w2v_model[word] for word in tokens if word in w2v_model]\n",
        "    return np.mean(vecs, axis=0) if vecs else np.zeros(300)\n",
        "\n",
        "w2v_embeddings = np.vstack([avg_word2vec(t) for t in tokenized_texts[:5]])\n",
        "print(\"Word2Vec Embeddings shape:\", w2v_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hN2DlUOAgtAq",
        "outputId": "1ffcf676-d8d4-4205-da04-8a1100181c7a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
            "Word2Vec Embeddings shape: (5, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: GloVe Embeddings\n",
        "!wget -q http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip\n",
        "\n",
        "glove_model = {}\n",
        "with open(\"glove.6B.100d.txt\", \"r\", encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        glove_model[values[0]] = np.asarray(values[1:], dtype='float32')\n",
        "\n",
        "def avg_glove(tokens):\n",
        "    vecs = [glove_model[w] for w in tokens if w in glove_model]\n",
        "    return np.mean(vecs, axis=0) if vecs else np.zeros(100)\n",
        "\n",
        "glove_embeddings = np.vstack([avg_glove(t) for t in tokenized_texts[:5]])\n",
        "print(\"GloVe Embeddings shape:\", glove_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C49Az3aDiK4a",
        "outputId": "df7c607c-68d3-4cf0-f4dc-0d7e4fccb230"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GloVe Embeddings shape: (5, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Pretrained FastText (Wiki News)\n",
        "ft_model = api.load(\"fasttext-wiki-news-subwords-300\")\n",
        "\n",
        "def avg_fasttext(tokens):\n",
        "    vecs = [ft_model[word] for word in tokens if word in ft_model]\n",
        "    return np.mean(vecs, axis=0) if vecs else np.zeros(300)\n",
        "\n",
        "fasttext_embeddings = np.vstack([avg_fasttext(t) for t in tokenized_texts[:5]])\n",
        "print(\"FastText Embeddings shape:\", fasttext_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7pW87fdjT1m",
        "outputId": "f046b959-d769-4d76-d1f7-30ff3dc3ba33"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 958.5/958.4MB downloaded\n",
            "FastText Embeddings shape: (5, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Project Summary and Conclusion\n",
        "This project focused on exploring various text embedding techniques to convert raw textual data into meaningful numerical representations. Using a subset of the IMDb movie reviews dataset, we implemented and compared classical and modern embedding methods to understand their characteristics and applicability.\n",
        "\n",
        "Data Preparation:\n",
        "The initial step involved preprocessing the textual data using the NLTK library, specifically the punkt tokenizer, which effectively segmented the text for further analysis.\n",
        "\n",
        "Embedding Techniques:\n",
        "\n",
        "Bag of Words (BoW) and TF-IDF: These foundational techniques transformed the corpus into sparse, high-dimensional vectors with a fixed vocabulary size of 5,000. While simple and interpretable, these methods do not capture semantic relationships or word context.\n",
        "\n",
        "Pretrained Word Embeddings (Word2Vec, GloVe, FastText): By leveraging pretrained models, we mapped words to dense vectors capturing semantic and syntactic properties. Word2Vec and FastText (both 300-dimensional) and GloVe (100-dimensional) embeddings provided richer and more compact representations compared to BoW/TF-IDF. Notably, FastText’s subword information helps represent out-of-vocabulary words effectively.\n",
        "\n",
        "Contextual Embeddings (BERT): Utilizing a transformer-based language model, we generated 768-dimensional contextual embeddings that consider the meaning of words within their sentence context, yielding superior performance in downstream tasks involving nuanced language understanding.\n",
        "\n",
        "Observations:\n",
        "The classical approaches (BoW and TF-IDF) are computationally efficient but limited in capturing deeper linguistic meaning. Pretrained embeddings bridge this gap by encoding semantic similarities. Contextual embeddings like BERT represent the state-of-the-art, effectively capturing context-dependent meanings at the cost of increased computational resources.\n",
        "\n",
        "Challenges:\n",
        "During the implementation, managing package dependencies (especially numpy and gensim) was crucial to avoid compatibility issues. Additionally, while Hugging Face’s token-based authentication warning appeared, it did not impede access to public datasets and models.\n",
        "\n",
        "Conclusion:\n",
        "This comprehensive exercise deepened our understanding of diverse embedding methodologies, highlighting the trade-offs between simplicity, computational requirements, and representational richness. The insights gained here are foundational for building robust natural language processing systems that can effectively analyze and interpret textual data.\n",
        "'''"
      ],
      "metadata": {
        "id": "_EtIMd8Yl6kq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}